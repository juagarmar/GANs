{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach to Histological Imagen Data Augmentation Using DCGAN  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Refereces:\n",
    "> - [Patching and Data Augmentation for Histological Images]()  \n",
    "> - [Simple DCNN for histological data classification]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index:\n",
    "> 1. Loading Packages and Data\n",
    "> 2. Get Batches\n",
    "> 3. Placeholders\n",
    "> 4. Discrimanator\n",
    "> 5. Generator\n",
    "> 6. Loss Function\n",
    "> 7. Adam Optimizer\n",
    "> 8. Train\n",
    "> 9. Run Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will load all the packages needed to perform this deeplearning technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is based in the one used in the tutorial \"Implementing a Generative Adversarial Network (GAN/DCGAN) to Draw Human Faces\".  \n",
    "\n",
    "Please see the link below:  \n",
    "https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=qbW-X6iW5jE\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/qbW-X6iW5jE/maxresdefault.jpg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the treated data performed in the notebook for \"Data Preparation and Augmentation\". For doing so we will use the function *pickle.load* as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"Train_Data.pickle\", \"rb\")\n",
    "Train_Data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to double check the type of normalisation we carried out in the previous section we calculate *min* and *max* of the database. as we can see, our data has been normalised from 0 to 1 values, what means we need to be careful with the type of activation function to be used in the final activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.007843138)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Train_Data),np.min(Train_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is with any doubt one of trickiest part of the code when we are trying to apply the model to our own data, as most of the training examples that can be found only make reference to the MNIST example, which is not very meaningful as it is pretreated data. In this senction we try to point out how to set the batches, which is not needed when working with the MNIST or CIFAR-10 examples.   \n",
    "  \n",
    "The code below will group sections of the matrix to split the database by batches, which is a requirement to apply DCGAN in most of the cases. This formula will be fixed to our database and the only variables that will be required would be tha batch size \"*batch_size*\" as the idea is change this variable to optimize this process as much as possible.  \n",
    "  \n",
    "As in the fucture we will use an *tanh* activation function or that will need to be normalised from -1 to 1. Therefore, will will centralised the data at the end of this step by applying *-0.5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size):\n",
    "    shape = len(Train_Data), IMAGE_WIDTH, IMAGE_HEIGHT, \n",
    "    data = Train_Data\n",
    "    \"\"\"\n",
    "    Generate batches\n",
    "    \"\"\"\n",
    "    current_index = 0\n",
    "    while current_index + batch_size <= shape[0]:\n",
    "        data_batch = data[current_index:current_index + batch_size]\n",
    "        current_index += batch_size\n",
    "        yield data_batch - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we will start using the tensorflow package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our tensorflow version by applying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very straightfoward step in our code. Placeholders are a tensorflow function that works as bridge between our numpy array data (float32) and tensorflow flow format (tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_real = tf.placeholder(tf.float32, shape=(None, image_width, image_height, image_channels), name='input_real') \n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return inputs_real, inputs_z, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Please notice that the name of the placeholder does not match the name of the variable where it is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a TensorFlow variable scope when defining this network. This helps us in the training process later so we can reuse our variable names for both the discriminator and the generator.\n",
    "\n",
    "The discriminator network consists of four convolutional layers. For every layer of the network, we are going to perform a convolution, then we are going to perform batch normalization to make the network faster and more accurate and finally, we are going to perform a Leaky ReLu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![alt text](https://d3ansictanv2wj.cloudfront.net/page4-515a902921add733ec6550b643b7e4d4.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False):\n",
    "    \"\"\"\n",
    "    Create the discriminator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # using 4 layer network as in DCGAN Paper\n",
    "        \n",
    "        # Conv 1\n",
    "        conv1 = tf.layers.conv2d(images, 64, 5, 2, 'SAME')\n",
    "        lrelu1 = tf.maximum(alpha * conv1, conv1)\n",
    "        \n",
    "        # Conv 2\n",
    "        conv2 = tf.layers.conv2d(lrelu1, 128, 5, 2, 'SAME')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training=True)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Conv 3\n",
    "        conv3 = tf.layers.conv2d(lrelu2, 256, 5, 1, 'SAME')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training=True)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "       \n",
    "        # Flatten\n",
    "        flat = tf.reshape(lrelu3, (-1, 4*4*256))\n",
    "        \n",
    "        # Logits\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        \n",
    "        # Output\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network consists of four deconvolutional layers. In here, we are doing the same as in the discriminator, just in the other direction. First, we take our input, called Z, and feed it into our first deconvolutional layer. Each deconvolutional layer performs a deconvolution and then performs batch normalization and a leaky ReLu as well. Then, we return the tanh activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![alt text](https://d3ansictanv2wj.cloudfront.net/page5-8bb6a0d5abd4b8660fb3c5d05ed20282.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse=False if is_train==True else True):\n",
    "        # First fully connected layer\n",
    "        x_1 = tf.layers.dense(z, 2*2*512)\n",
    "        \n",
    "        # Reshape it to start the convolutional stack\n",
    "        deconv_2 = tf.reshape(x_1, (-1, 2, 2, 512))\n",
    "        batch_norm2 = tf.layers.batch_normalization(deconv_2, training=is_train)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Deconv 1\n",
    "        deconv3 = tf.layers.conv2d_transpose(lrelu2, 256, 5, 2, padding='VALID')\n",
    "        batch_norm3 = tf.layers.batch_normalization(deconv3, training=is_train)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "        \n",
    "        \n",
    "        # Deconv 2\n",
    "        deconv4 = tf.layers.conv2d_transpose(lrelu3, 128, 5, 2, padding='SAME')\n",
    "        batch_norm4 = tf.layers.batch_normalization(deconv4, training=is_train)\n",
    "        lrelu4 = tf.maximum(alpha * batch_norm4, batch_norm4)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.conv2d_transpose(lrelu4, out_channel_dim, 5, 2, padding='SAME')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just having a single loss function, we need to define three: The loss of the generator, the loss of the discriminator when using real images and the loss of the discriminator when using fake images. The sum of the fake image and real image loss is the overall discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    \"\"\"\n",
    "    \n",
    "    label_smoothing = 0.9\n",
    "    \n",
    "    g_model = generator(input_z, out_channel_dim)\n",
    "    d_model_real, d_logits_real = discriminator(input_real)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                labels=tf.ones_like(d_model_real) * label_smoothing))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "                                                  \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.ones_like(d_model_fake) * label_smoothing))\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than just having a single loss function, we need to define three: The loss of the generator, the loss of the discriminator when using real images and the loss of the discriminator when using fake images. The sum of the fake image and real image loss is the overall discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    \"\"\"\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of our preparation, we are writing a small helper function to display the generated images in the notebook for us, using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generator_output(sess, n_images, input_z, out_channel_dim):\n",
    "    \"\"\"\n",
    "    Show example output for the generator\n",
    "    \"\"\"\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "    #plt.imshow(helper.images_square_grid(samples))\n",
    "    #plt.show()\n",
    "\n",
    "    plt.imshow(np.array((((samples[0])/2)+0.5), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just get our inputs, losses and optimizers which we defined before, call a TensorFlow session and run it batch per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "z_dim = 100\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "IMAGE_WIDTH = 200\n",
    "IMAGE_HEIGHT = 200\n",
    "shape = len(Train_Data), IMAGE_WIDTH, IMAGE_HEIGHT, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One off as reuse is false\n",
    "input_real, input_z, _ = model_inputs(shape[1], shape[2], shape[3], z_dim)\n",
    "d_loss, g_loss = model_loss(input_real, input_z, shape[3])\n",
    "d_opt, g_opt = model_opt(d_loss, g_loss, learning_rate, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2... Discriminator Loss: 0.4942... Generator Loss: 1.9007\n",
      "###########################################################\n",
      "Tensor(\"input_z:0\", shape=(?, 100), dtype=float32) 3\n",
      "Epoch 2/2... Discriminator Loss: 0.3664... Generator Loss: 3.1264\n",
      "###########################################################\n",
      "Tensor(\"input_z:0\", shape=(?, 100), dtype=float32) 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 2\n",
    "epoch_i = 0\n",
    "steps = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_images in get_batches(batch_size):\n",
    "            batch_images = batch_images * 2\n",
    "            steps += 1\n",
    "            #batch_z = np.array(np.random.uniform(-1, 1, size=(batch_size,z_dim)), np.float32)\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size,z_dim))\n",
    "            _ = sess.run(d_opt, feed_dict={input_real: batch_images, input_z: np.array(batch_z, np.float32)})\n",
    "            _ = sess.run(g_opt, feed_dict={input_real: batch_images, input_z: np.array(batch_z, np.float32)})  \n",
    "            #print(\"here!\")\n",
    "            if steps % 83 == 0:\n",
    "                #print(\"There!\")\n",
    "                # At the end of every 10 epochs, get the losses and print them out\n",
    "                train_loss_d = d_loss.eval({input_z: batch_z, input_real: batch_images})\n",
    "                train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "                \n",
    "                _ = show_generator_output(sess, 1, input_z, shape[3])\n",
    "\n",
    "                print(\"Epoch {}/{}...\".format(epoch_i+1, epochs),\n",
    "                        \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                        \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                print(\"###########################################################\")\n",
    "                print (input_z, shape[3])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_generator_output(sess, n_images, input_z, out_channel_dim):\n",
    "z_dim = 0\n",
    "inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    show_generator_output(sess, 1, input_z, shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
